{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import floor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <eos> = Valkyria Chronicles III = <eos> <eos> Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story \n"
     ]
    }
   ],
   "source": [
    "with open('wikitext-2/wiki.train.tokens') as f:\n",
    "    data = f.read()\n",
    "data = data.replace('\\n', '<eos>') #suggested preprocessing from source\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 113161), (',', 99913), ('.', 73388), ('of', 56889), ('<unk>', 54625)]\n",
      "\n",
      " Number of unique tokens: 33278\n"
     ]
    }
   ],
   "source": [
    "split = data.split()\n",
    "tokens = Counter(split)\n",
    "print(tokens.most_common()[:5])\n",
    "\n",
    "VOCAB_SIZE = len(tokens)\n",
    "print(f'\\n Number of unique tokens: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eos>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "stoi, itos = {}, {}\n",
    "for i, (tok, _) in enumerate(tokens.items()):\n",
    "    stoi[tok] = i; itos[i] = tok\n",
    "#end for\n",
    "\n",
    "print(itos[0])\n",
    "print(stoi['<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have integer representations of all unique tokens in the dataset. (This could have been done with the native [kears tokenizer](https://keras.io/preprocessing/text/#tokenizer) , but since the dataset was already cleaned for us, I guess we could still achive similar results). The most simple approach to turn these into vectors is to use the integers as a one hot encoding paramter. This approch is however quite poor since all words in this embedding-space are equidistant. A better approach that I will try later is to use a dence representation, which we will include in the training of our network, where each word share features with eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_data = to_categorical([stoi[tok] for tok in split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33278\n",
      "2088628\n"
     ]
    }
   ],
   "source": [
    "print(len(oh_data[0])) #one-hot representation\n",
    "print(len(oh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "n_seq = floor(len(oh_data)/seq_len)\n",
    "n_seq = 20\n",
    "em_sz = len(oh_data[0])\n",
    "\n",
    "#Create each training sequence\n",
    "X_train, Y_train = np.zeros((n_seq, seq_len, em_sz)), np.zeros((n_seq, seq_len, em_sz))\n",
    "for i in range(n_seq):\n",
    "    X_train[i,:,:] = oh_data[i*seq_len:(i+1)*seq_len]\n",
    "    Y_train[i,:,:] = oh_data[i*seq_len+1:(i+1)*seq_len+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate randomly shuffeling our training data, we cannot change the order of our token, but we can change the length or input sequences. This is something that we need to implement later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to feed this data into a model. The goal here is to predict the next word given a set of previous ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 10\n",
    "n_layer = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(h_dim, input_shape=(None, em_sz), return_sequences=True))\n",
    "for i in range(n_layer - 1):\n",
    "    model.add(LSTM(h_dim, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(em_sz)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 0\n",
    "bs = 2\n",
    "while True:\n",
    "    print('\\n\\n')\n",
    "    model.fit(X_train, Y_train, batch_size=bs, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH)\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
